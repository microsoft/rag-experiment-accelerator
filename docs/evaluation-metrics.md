# RAG Answer Evaluation Metrics

The following is an overview of the available evaluation metrics that can be used to evaluate end-to-end performance of
a RAG application by measuring a distance between the ground truth answer and the predicted answer.

These metrics are calculated as part of  the `04_evaluation.py` script based on the `actual`, `expected` and `context` fields of
the `.jsonl` output file (referred to as "calculation base"), generated by `03_querying.py` script. See the [script inputs and outputs
guide](/docs/script-inputs-outputs.md#03_queryingpy) for more information.

You can choose which metrics should be calculated in your experiment by updating the `metric_types` field in the
`search_config.json` configuration file.

## Configuration Example

```json
"metric_types": [
    "lcsstr",
    "lcsseq",
    "cosine",
    "jaro_winkler",
    "hamming",
    "jaccard",
    "levenshtein",
    "fuzzy",
    "bert_all_MiniLM_L6_v2",
    "bert_base_nli_mean_tokens",
    "bert_large_nli_mean_tokens",
    "bert_large_nli_stsb_mean_tokens",
    "bert_distilbert_base_nli_stsb_mean_tokens",
    "bert_paraphrase_multilingual_MiniLM_L12_v2",
    "answer_relevance",
    "context_precision"
]
```

## Available Metrics

### Longest common substring

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `lcsstr`          | `actual`, `expected` | Percentage (0-100) |

Calculates the longest common substring (LCS) similarity score between two strings.

### Longest common subsequence

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `lcsseq`          | `actual`, `expected` | Percentage (0-100) |

Computes the longest common subsequence (LCS) similarity score between two input strings.

### Cosine similarity

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `cosine`          | `actual`, `expected` | Percentage (0-100) |

The cosine similarity is calculated as the cosine of the angle between the two vectors of the strings.

### Jaro-Winkler distance

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `jaro_winkler`    | `actual`, `expected` | Percentage (0-100) |

The Jaro-Winkler similarity score is a measure of similarity between two strings. The Jaro-Winkler similarity score is
calculated as the number of characters that are different between the two strings divided by the number of characters
that are the same between the two strings.

### Hamming distance

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `hamming`         | `actual`, `expected` | Percentage (0-100) |

The Hamming distance is a measure of similarity between two strings. The Hamming distance is calculated as the number of
characters that are different between the two strings.

### Jaccard similarity

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `jaccard`         | `actual`, `expected` | Percentage (0-100) |

The Jaccard similarity is calculated as the number of elements in the intersection of the two sets divided by the number
of elements in the union of the two sets.

### Levenshtein distance

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `levenshtein`     | `actual`, `expected` | Percentage (0-100) |

The Levenshtein distance is a measure of similarity between two strings. The Levenshtein distance is calculated as the
minimum number of insertions, deletions, or substitutions required to transform one string into the other.

### FuzzyWuzzy similarity

| Configuration Key | Calculation Base     | Possible Values       |
| ----------------- | -------------------- | --------------------- |
| `fuzzy`           | `actual`, `expected` | Integer (Fuzzy score) |

Calculates the fuzzy score between two documents using the levenshtein distance.

### BERT-based semantic similarity

Calculates semantic similarity between two strings as percentage of differences based on embeddings generated by
different BERT models.

| Calculation Base     | Possible Values    |
| -------------------- | ------------------ |
| `actual`, `expected` | Percentage (0-100) |

| Configuration Key                          | BERT Model                                   |
| ------------------------------------------ | -------------------------------------------- |
| bert_all_MiniLM_L6_v2                      | MiniLM L6 v2 model                           |
| bert_base_nli_mean_tokens                  | Base model, mean tokens                      |
| bert_large_nli_mean_tokens                 | Large model, mean tokens                     |
| bert_large_nli_stsb_mean_tokens            | Large model, STS-B, mean tokens              |
| bert_distilbert_base_nli_stsb_mean_tokens  | DistilBERT base model, STS-B, mean tokens    |
| bert_paraphrase_multilingual_MiniLM_L12_v2 | Multilingual paraphrase model, MiniLM L12 v2 |

### Answer relevance

| Configuration Key  | Calculation Base     | Possible Values                   |
| ------------------ | -------------------- | --------------------------------- |
| `answer_relevance` | `actual`, `expected` | From 0 to 1 with 1 being the best |

Scores the relevancy of the answer according to the given question. Answers with incomplete, redundant or unnecessary
information is penalized.

### Context precision

Configuraion key: `context_precision`

| Configuration Key   | Calculation Base     | Possible Values                                        |
| ------------------- | -------------------- | ------------------------------------------------------ |
| `context_precision` | `actual`, `context` | 1 or 0 depending on if the context is relevant or not. |

Checks whether or not the context generated by the RAG solution is useful for answering a question.
