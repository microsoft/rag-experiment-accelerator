# Evaluation Metrics

The following is an overview of the available evaluation metrics that can be used to evaluate end-to-end performance of
a RAG application by measuring a distance between the ground truth answer and the predicted answer.

These metrics are calculated as part of  the `04_evaluation.py` script based on the `actual`, `expected` and `context` fields of
the `.jsonl` output file (referred to as "calculation base"), generated by `03_querying.py` script. See the [script inputs and outputs
guide](/docs/script-inputs-outputs.md#03_queryingpy) for more information.

You can choose which metrics should be calculated in your experiment by updating the `metric_types` field in the
`search_config.json` configuration file.

## Configuration Example

```json
"metric_types": [
    "lcsstr",
    "lcsseq",
    "cosine",
    "jaro_winkler",
    "hamming",
    "jaccard",
    "levenshtein",
    "fuzzy",
    "bert_all_MiniLM_L6_v2",
    "bert_base_nli_mean_tokens",
    "bert_large_nli_mean_tokens",
    "bert_large_nli_stsb_mean_tokens",
    "bert_distilbert_base_nli_stsb_mean_tokens",
    "bert_paraphrase_multilingual_MiniLM_L12_v2",
    "llm_answer_relevance",
    "llm_context_precision"
]
```

## Algorithm-based Metrics

The following metrics are calculated by using different string similarity algorithms mostly backed by the [TextDistance
Python package](https://pypi.org/project/textdistance/).

### Longest common substring

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `lcsstr`          | `actual`, `expected` | Percentage (0-100) |

Calculates the longest common substring (LCS) similarity score between two strings.

### Longest common subsequence

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `lcsseq`          | `actual`, `expected` | Percentage (0-100) |

Computes the longest common subsequence (LCS) similarity score between two input strings.

### Cosine similarity (Ochiai coefficient)

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `cosine`          | `actual`, `expected` | Percentage (0-100) |

This coefficient is calculated as the intersection of the term-frequency vectors of the generated answer (actual) and the ground-truth answer (expected) divided by the geometric mean of the sizes of these vectors.

### Jaro-Winkler distance

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `jaro_winkler`    | `actual`, `expected` | Percentage (0-100) |

The Jaro-Winkler similarity score is a measure of similarity between two strings. The Jaro-Winkler similarity score is
calculated as the number of characters that are different between the two strings divided by the number of characters
that are the same between the two strings.

### Hamming distance

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `hamming`         | `actual`, `expected` | Percentage (0-100) |

The Hamming distance is a measure of similarity between two strings. The Hamming distance is calculated as the number of
characters that are different between the two strings.

### Jaccard similarity

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `jaccard`         | `actual`, `expected` | Percentage (0-100) |

The Jaccard similarity is calculated as the number of elements in the intersection of the two sets divided by the number
of elements in the union of the two sets.

### Levenshtein distance

| Configuration Key | Calculation Base     | Possible Values    |
| ----------------- | -------------------- | ------------------ |
| `levenshtein`     | `actual`, `expected` | Percentage (0-100) |

The Levenshtein distance is a measure of similarity between two strings. The Levenshtein distance is calculated as the
minimum number of insertions, deletions, or substitutions required to transform one string into the other.

### FuzzyWuzzy similarity

| Configuration Key | Calculation Base     | Possible Values       |
| ----------------- | -------------------- | --------------------- |
| `fuzzy`           | `actual`, `expected` | Integer (Fuzzy score) |

This metric is backed by the [FuzzyWuzzy Python package](https://pypi.org/project/fuzzywuzzy/).
Calculates the fuzzy score between two documents using the levenshtein distance.

## BERT-based semantic similarity

The following set of metrics calculates semantic similarity between two strings as percentage of differences based on
embeddings created by different BERT models. Backed by the [sentence-transformers Python
package](https://pypi.org/project/sentence-transformers/).

| Calculation Base     | Possible Values    |
| -------------------- | ------------------ |
| `actual`, `expected` | Percentage (0-100) |

| Configuration Key                          | BERT Model                                   |
| ------------------------------------------ | -------------------------------------------- |
| bert_all_MiniLM_L6_v2                      | MiniLM L6 v2 model                           |
| bert_base_nli_mean_tokens                  | Base model, mean tokens                      |
| bert_large_nli_mean_tokens                 | Large model, mean tokens                     |
| bert_large_nli_stsb_mean_tokens            | Large model, STS-B, mean tokens              |
| bert_distilbert_base_nli_stsb_mean_tokens  | DistilBERT base model, STS-B, mean tokens    |
| bert_paraphrase_multilingual_MiniLM_L12_v2 | Multilingual paraphrase model, MiniLM L12 v2 |

## LLM-based Metrics

The following metrics are calculated based on LLM reasoning. These metrics require the OpenAI endpoint to be configured
(see [Environment Variables](./environment-variables.md)).

These metrics also require the `chat_model_name` property to be set in the `search_config.json` configuration file. See
[Description of configuration elements](../README.md#description-of-configuration-elements) for details.

### LLM Answer relevance

| Configuration Key  | Calculation Base     | Possible Values                   |
| ------------------ | -------------------- | --------------------------------- |
| `llm_answer_relevance` | `actual`, `expected` | From 0 to 1 with 1 being the best |

Scores the relevancy of the answer according to the given question. Answers with incomplete, redundant or unnecessary
information is penalized.

### LLM Context precision

| Configuration Key   | Calculation Base    | Possible Values                                                   |
| ------------------- | ------------------- | ----------------------------------------------------------------- |
| `llm_context_precision` | `actual`, `context` | 1 (yes) or 0 (no) depending on if the context is relevant or not. |

Checks whether or not the context generated by the RAG solution is useful for answering a question.
